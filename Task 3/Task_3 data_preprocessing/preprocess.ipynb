{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for city D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "mobility_data = pd.read_csv(\"kumamoto_challengedata.csv\")\n",
    "\n",
    "# Load POI distribution data\n",
    "poi_data = pd.read_csv(\"POIdata_cityD.csv\")\n",
    "\n",
    "# Load POI category mappings\n",
    "poi_categories = pd.read_csv(\"POI_datacategories.csv\", header=None, names=['category_name'])\n",
    "poi_categories['category_id'] = range(1, len(poi_categories) + 1)\n",
    "\n",
    "# Load Task 1 output (frequent itemsets)\n",
    "task1_output = pd.read_csv(\"frequent_itemsets_cityD.csv\")\n",
    "\n",
    "# Load Task 2 output (frequent patterns)\n",
    "task2_patterns = pd.read_csv('kumamoto_freq_subseq.csv')['Patterns'].apply(eval).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge POI categories with distribution data\n",
    "poi_data = pd.merge(poi_data, poi_categories, left_on='category', right_on='category_id', how='left')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter mobility data to the first 30 days\n",
    "mobility_data['d'] = pd.to_numeric(mobility_data['d'], errors='coerce')\n",
    "mobility_data = mobility_data[mobility_data['d'] <= 30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove rows with missing coordinates\n",
    "mobility_data = mobility_data[~((mobility_data['x'] == -999) & (mobility_data['y'] == -999))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate frequent support for each POI using Task 1\n",
    "def calculate_frequent_support(poi_category, task1_data):\n",
    "    return task1_data.loc[\n",
    "        task1_data['itemsets'].apply(lambda s: poi_category in s), 'support'\n",
    "    ].sum()\n",
    "\n",
    "poi_data['frequent_support'] = poi_data['category_name'].apply(\n",
    "    lambda x: calculate_frequent_support(x, task1_output)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate POI data by grid\n",
    "poi_aggregated = poi_data.groupby(['x', 'y']).agg(\n",
    "    total_poi_count=('POI_count', 'sum'),\n",
    "    avg_frequent_support=('frequent_support', 'mean')\n",
    ").reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge POI features with mobility data\n",
    "mobility_data = pd.merge(\n",
    "    mobility_data, poi_aggregated,\n",
    "    on=['x', 'y'], how='left'\n",
    ").fillna(0)  # Fill missing POI features with 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Precompute POI features as a NumPy array\n",
    "poi_features = ['total_poi_count', 'avg_frequent_support']\n",
    "poi_features_array = mobility_data[poi_features].to_numpy()\n",
    "\n",
    "# Sort the entire dataset once by user and time\n",
    "mobility_data = mobility_data.sort_values(['uid', 'd', 't']).reset_index(drop=True)\n",
    "\n",
    "# Convert task2_patterns to a set of tuples for fast lookup\n",
    "task2_patterns_set = set(map(tuple, task2_patterns))\n",
    "\n",
    "# Function to check if a sequence matches a pattern (optimized for set lookup)\n",
    "def match_pattern(sequence, patterns_set):\n",
    "    return 1 if tuple(map(tuple, sequence)) in patterns_set else 0\n",
    "\n",
    "# Function to process a single sequence for multiprocessing\n",
    "def process_sequence(data):\n",
    "    seq, match, poi_features_array = data\n",
    "    enriched_seq = [\n",
    "        np.hstack([step, poi_features_array[i], match]) for i, step in enumerate(seq)\n",
    "    ]\n",
    "    return enriched_seq\n",
    "\n",
    "# Group by user\n",
    "grouped = mobility_data.groupby('uid')\n",
    "\n",
    "# Initialize list for enhanced sequences\n",
    "enhanced_sequences = []\n",
    "\n",
    "# Sequence length for LSTM\n",
    "sequence_length = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing User Groups: 100%|██████████| 5983/5983 [00:14<00:00, 405.87it/s]\n"
     ]
    }
   ],
   "source": [
    "# Process each group with tqdm for progress tracking\n",
    "for _, group in tqdm(grouped, desc=\"Processing User Groups\"):\n",
    "    # Convert group to NumPy array for faster slicing\n",
    "    group_array = group[['x', 'y']].to_numpy()\n",
    "\n",
    "    # Generate sequences\n",
    "    num_sequences = len(group_array) - sequence_length\n",
    "    if num_sequences > 0:\n",
    "        sequences = np.lib.stride_tricks.sliding_window_view(group_array, (sequence_length, group_array.shape[1]))[:, 0]\n",
    "        matches = [match_pattern(seq, task2_patterns_set) for seq in sequences]\n",
    "        enhanced_sequences.extend(zip(sequences, matches))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize features\n",
    "max_x, max_y = mobility_data['x'].max(), mobility_data['y'].max()\n",
    "mobility_data['x'] = mobility_data['x'] / max_x\n",
    "mobility_data['y'] = mobility_data['y'] / max_y\n",
    "\n",
    "poi_features = ['total_poi_count', 'avg_frequent_support']\n",
    "mobility_data[poi_features] = mobility_data[poi_features] / mobility_data[poi_features].max()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating LSTM Sequences: 100%|██████████| 3452950/3452950 [01:24<00:00, 40635.23it/s]\n"
     ]
    }
   ],
   "source": [
    "# Updated Loop\n",
    "final_sequences = []\n",
    "for seq, match in tqdm(enhanced_sequences, desc=\"Creating LSTM Sequences\"):\n",
    "    # Combine sequence with precomputed POI features and pattern match\n",
    "    enriched_seq = [\n",
    "        np.hstack([step, poi_features_array[i], match])  # Use preloaded features\n",
    "        for i, step in enumerate(seq)\n",
    "    ]\n",
    "    final_sequences.append(enriched_seq)\n",
    "\n",
    "final_sequences = np.array(final_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez('city_D_processed_data_optimized.npz', sequences=final_sequences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 1 output = frequent itemsets to calculate frequent_support for each grid (x, y).\n",
    "\n",
    "Task 2 output = frequent sequence Add a feature to indicate whether a user sequence matches any frequent pattern.\n",
    "\n",
    "Filtered the main dataset to 30 days\n",
    "\n",
    "Combine the enriched POI features and pattern matches with the main data.\n",
    "Prepare sequences of fixed length for LSTM training.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for city a\n",
    "\n",
    "# load data\n",
    "mobility_data = pd.read_csv(\"task1_dataset_kotae.csv\")\n",
    "\n",
    "# Load POI distribution data\n",
    "poi_data = pd.read_csv(\"POIdata_cityA.csv\")\n",
    "\n",
    "# Load POI category mappings\n",
    "poi_categories = pd.read_csv(\"POI_datacategories.csv\", header=None, names=['category_name'])\n",
    "poi_categories['category_id'] = range(1, len(poi_categories) + 1)\n",
    "\n",
    "# Load Task 1 output (frequent itemsets)\n",
    "task1_output = pd.read_csv(\"frequent_itemsets_cityA.csv\")\n",
    "\n",
    "# Load Task 2 output (frequent patterns)\n",
    "task2_patterns = pd.read_csv('kotae_freq_subseq.csv')['Patterns'].apply(eval).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge POI categories with distribution data\n",
    "poi_data = pd.merge(poi_data, poi_categories, left_on='category', right_on='category_id', how='left')\n",
    "\n",
    "# Filter mobility data to the first 30 days\n",
    "mobility_data['d'] = pd.to_numeric(mobility_data['d'], errors='coerce')\n",
    "mobility_data = mobility_data[mobility_data['d'] <= 30]\n",
    "\n",
    "# Remove rows with missing coordinates\n",
    "mobility_data = mobility_data[~((mobility_data['x'] == -999) & (mobility_data['y'] == -999))]\n",
    "\n",
    "# Calculate frequent support for each POI using Task 1\n",
    "def calculate_frequent_support(poi_category, task1_data):\n",
    "    return task1_data.loc[\n",
    "        task1_data['itemsets'].apply(lambda s: poi_category in s), 'support'\n",
    "    ].sum()\n",
    "\n",
    "poi_data['frequent_support'] = poi_data['category_name'].apply(\n",
    "    lambda x: calculate_frequent_support(x, task1_output)\n",
    ")\n",
    "\n",
    "# Aggregate POI data by grid\n",
    "poi_aggregated = poi_data.groupby(['x', 'y']).agg(\n",
    "    total_poi_count=('POI_count', 'sum'),\n",
    "    avg_frequent_support=('frequent_support', 'mean')\n",
    ").reset_index()\n",
    "\n",
    "# Merge POI features with mobility data\n",
    "mobility_data = pd.merge(\n",
    "    mobility_data, poi_aggregated,\n",
    "    on=['x', 'y'], how='left'\n",
    ").fillna(0)  # Fill missing POI features with 0\n",
    "\n",
    "# Precompute POI features as a NumPy array\n",
    "poi_features = ['total_poi_count', 'avg_frequent_support']\n",
    "poi_features_array = mobility_data[poi_features].to_numpy()\n",
    "\n",
    "# Sort the entire dataset once by user and time\n",
    "mobility_data = mobility_data.sort_values(['uid', 'd', 't']).reset_index(drop=True)\n",
    "\n",
    "# Convert task2_patterns to a set of tuples for fast lookup\n",
    "task2_patterns_set = set(map(tuple, task2_patterns))\n",
    "\n",
    "# Function to check if a sequence matches a pattern (optimized for set lookup)\n",
    "def match_pattern(sequence, patterns_set):\n",
    "    return 1 if tuple(map(tuple, sequence)) in patterns_set else 0\n",
    "\n",
    "# Function to process a single sequence for multiprocessing\n",
    "def process_sequence(data):\n",
    "    seq, match, poi_features_array = data\n",
    "    enriched_seq = [\n",
    "        np.hstack([step, poi_features_array[i], match]) for i, step in enumerate(seq)\n",
    "    ]\n",
    "    return enriched_seq\n",
    "\n",
    "# Group by user\n",
    "grouped = mobility_data.groupby('uid')\n",
    "\n",
    "# Initialize list for enhanced sequences\n",
    "enhanced_sequences = []\n",
    "# Sequence length for LSTM\n",
    "sequence_length = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing User Groups: 100%|██████████| 99773/99773 [03:11<00:00, 521.42it/s]\n"
     ]
    }
   ],
   "source": [
    "# Process each group with tqdm for progress tracking\n",
    "for _, group in tqdm(grouped, desc=\"Processing User Groups\"):\n",
    "    # Convert group to NumPy array for faster slicing\n",
    "    group_array = group[['x', 'y']].to_numpy()\n",
    "\n",
    "    # Generate sequences\n",
    "    num_sequences = len(group_array) - sequence_length\n",
    "    if num_sequences > 0:\n",
    "        sequences = np.lib.stride_tricks.sliding_window_view(group_array, (sequence_length, group_array.shape[1]))[:, 0]\n",
    "        matches = [match_pattern(seq, task2_patterns_set) for seq in sequences]\n",
    "        enhanced_sequences.extend(zip(sequences, matches))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize features\n",
    "max_x, max_y = mobility_data['x'].max(), mobility_data['y'].max()\n",
    "mobility_data['x'] = mobility_data['x'] / max_x\n",
    "mobility_data['y'] = mobility_data['y'] / max_y\n",
    "\n",
    "poi_features = ['total_poi_count', 'avg_frequent_support']\n",
    "mobility_data[poi_features] = mobility_data[poi_features] / mobility_data[poi_features].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating LSTM Sequences:   2%|▏         | 1009490/44382897 [00:29<1:39:28, 7267.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0 saved with 1000000 sequences.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating LSTM Sequences:   5%|▍         | 2005535/44382897 [00:57<1:57:00, 6035.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1 saved with 1000000 sequences.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating LSTM Sequences:   7%|▋         | 3007823/44382897 [01:26<1:46:36, 6468.48it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 2 saved with 1000000 sequences.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating LSTM Sequences:   9%|▉         | 4008766/44382897 [01:54<1:20:03, 8404.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 3 saved with 1000000 sequences.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating LSTM Sequences:  11%|█▏        | 5007325/44382897 [02:22<1:42:17, 6415.92it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 4 saved with 1000000 sequences.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating LSTM Sequences:  14%|█▎        | 6009035/44382897 [02:51<1:13:16, 8727.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 5 saved with 1000000 sequences.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating LSTM Sequences:  16%|█▌        | 7001513/44382897 [03:18<2:12:50, 4690.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 6 saved with 1000000 sequences.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating LSTM Sequences:  18%|█▊        | 8009134/44382897 [03:47<1:09:15, 8753.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 7 saved with 1000000 sequences.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating LSTM Sequences:  20%|██        | 9008313/44382897 [04:15<1:09:01, 8540.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 8 saved with 1000000 sequences.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating LSTM Sequences:  23%|██▎       | 10005595/44382897 [04:43<1:26:14, 6643.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 9 saved with 1000000 sequences.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating LSTM Sequences:  25%|██▍       | 11006910/44382897 [05:10<1:29:12, 6235.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 10 saved with 1000000 sequences.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating LSTM Sequences:  27%|██▋       | 12006427/44382897 [05:39<1:24:42, 6370.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 11 saved with 1000000 sequences.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating LSTM Sequences:  29%|██▉       | 13005257/44382897 [06:08<1:20:51, 6467.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 12 saved with 1000000 sequences.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating LSTM Sequences:  32%|███▏      | 14005122/44382897 [06:36<1:16:48, 6590.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 13 saved with 1000000 sequences.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating LSTM Sequences:  34%|███▍      | 15007848/44382897 [07:04<1:15:40, 6469.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 14 saved with 1000000 sequences.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating LSTM Sequences:  36%|███▌      | 16009571/44382897 [07:32<55:30, 8520.19it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 15 saved with 1000000 sequences.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating LSTM Sequences:  38%|███▊      | 17007879/44382897 [08:00<1:08:29, 6661.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 16 saved with 1000000 sequences.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating LSTM Sequences:  41%|████      | 18005607/44382897 [08:28<1:09:51, 6292.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 17 saved with 1000000 sequences.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating LSTM Sequences:  43%|████▎     | 19007698/44382897 [08:55<1:04:11, 6587.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 18 saved with 1000000 sequences.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating LSTM Sequences:  45%|████▌     | 20009012/44382897 [09:24<48:29, 8377.31it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 19 saved with 1000000 sequences.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating LSTM Sequences:  47%|████▋     | 21005985/44382897 [09:51<57:20, 6794.30it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 20 saved with 1000000 sequences.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating LSTM Sequences:  50%|████▉     | 22004690/44382897 [10:19<58:11, 6409.88it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 21 saved with 1000000 sequences.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating LSTM Sequences:  52%|█████▏    | 23006772/44382897 [10:46<53:44, 6628.91it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 22 saved with 1000000 sequences.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating LSTM Sequences:  54%|█████▍    | 24004093/44382897 [11:15<59:41, 5690.15it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 23 saved with 1000000 sequences.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating LSTM Sequences:  56%|█████▋    | 25005020/44382897 [11:42<56:37, 5704.29it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 24 saved with 1000000 sequences.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating LSTM Sequences:  59%|█████▊    | 26006325/44382897 [12:10<47:45, 6413.89it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 25 saved with 1000000 sequences.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating LSTM Sequences:  61%|██████    | 27004405/44382897 [12:39<47:26, 6105.05it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 26 saved with 1000000 sequences.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating LSTM Sequences:  63%|██████▎   | 28006009/44382897 [13:07<48:25, 5636.94it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 27 saved with 1000000 sequences.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating LSTM Sequences:  65%|██████▌   | 29002844/44382897 [13:35<55:11, 4644.61it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 28 saved with 1000000 sequences.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating LSTM Sequences:  68%|██████▊   | 30004925/44382897 [14:04<37:45, 6347.76it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 29 saved with 1000000 sequences.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating LSTM Sequences:  70%|██████▉   | 31005446/44382897 [14:31<34:40, 6428.49it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 30 saved with 1000000 sequences.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating LSTM Sequences:  72%|███████▏  | 32006177/44382897 [14:59<30:00, 6872.78it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 31 saved with 1000000 sequences.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating LSTM Sequences:  74%|███████▍  | 33008477/44382897 [15:26<27:55, 6787.65it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 32 saved with 1000000 sequences.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating LSTM Sequences:  77%|███████▋  | 34007761/44382897 [15:54<25:03, 6900.59it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 33 saved with 1000000 sequences.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating LSTM Sequences:  79%|███████▉  | 35006099/44382897 [16:21<23:23, 6680.85it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 34 saved with 1000000 sequences.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating LSTM Sequences:  81%|████████  | 36005854/44382897 [16:49<21:03, 6628.34it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 35 saved with 1000000 sequences.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating LSTM Sequences:  83%|████████▎ | 37005639/44382897 [17:17<18:57, 6484.27it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 36 saved with 1000000 sequences.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating LSTM Sequences:  86%|████████▌ | 38004742/44382897 [17:45<16:14, 6544.15it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 37 saved with 1000000 sequences.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating LSTM Sequences:  88%|████████▊ | 39007478/44382897 [18:14<14:10, 6318.86it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 38 saved with 1000000 sequences.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating LSTM Sequences:  90%|█████████ | 40006823/44382897 [18:41<11:08, 6544.78it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 39 saved with 1000000 sequences.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating LSTM Sequences:  92%|█████████▏| 41008233/44382897 [19:09<08:34, 6562.13it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 40 saved with 1000000 sequences.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating LSTM Sequences:  95%|█████████▍| 42006356/44382897 [19:37<06:00, 6590.51it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 41 saved with 1000000 sequences.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating LSTM Sequences:  97%|█████████▋| 43007482/44382897 [20:05<03:35, 6373.35it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 42 saved with 1000000 sequences.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating LSTM Sequences:  99%|█████████▉| 44006045/44382897 [20:33<00:57, 6536.58it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 43 saved with 1000000 sequences.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating LSTM Sequences: 100%|██████████| 44382897/44382897 [20:42<00:00, 35717.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final batch 44 saved with 382897 sequences.\n"
     ]
    }
   ],
   "source": [
    "batch_size = 1000000  # Adjust based on available memory\n",
    "final_sequences = []  # Temporary storage for the current batch\n",
    "batch_index = 0  # To track batch numbers\n",
    "\n",
    "for idx, (seq, match) in enumerate(tqdm(enhanced_sequences, desc=\"Creating LSTM Sequences\")):\n",
    "    # Process the current sequence\n",
    "    enriched_seq = [\n",
    "        np.hstack([step, poi_features_array[i], match])  # Combine step with features\n",
    "        for i, step in enumerate(seq)\n",
    "    ]\n",
    "    final_sequences.append(enriched_seq)\n",
    "\n",
    "    \n",
    "    if (idx + 1) % batch_size == 0:\n",
    "        # Convert batch to NumPy array\n",
    "        sequences = np.array(final_sequences)\n",
    "        \n",
    "        # Save batch to disk\n",
    "        np.savez(f'city_A_batch_{batch_index}.npz', sequences=sequences)\n",
    "        print(f\"Batch {batch_index} saved with {len(final_sequences)} sequences.\")\n",
    "        \n",
    "        # Reset for the next batch\n",
    "        final_sequences = []\n",
    "        batch_index += 1\n",
    "\n",
    "# Save the remaining sequences as the last batch\n",
    "if final_sequences:\n",
    "    sequences = np.array(final_sequences)\n",
    "    np.savez(f'city_A_batch_{batch_index}.npz', sequences=sequences)\n",
    "    print(f\"Final batch {batch_index} saved with {len(final_sequences)} sequences.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All batches combined and saved.\n"
     ]
    }
   ],
   "source": [
    "combined_sequences = []\n",
    "\n",
    "for i in range(batch_index + 1):  # Includes the final batch\n",
    "    with np.load(f'city_A_batch_{i}.npz') as data:\n",
    "        combined_sequences.extend(data['sequences'])\n",
    "\n",
    "# Convert to a single NumPy array if needed\n",
    "combined_sequences = np.array(combined_sequences)\n",
    "np.savez('city_A_processed_data_optimized.npz', sequences=combined_sequences)\n",
    "print(\"All batches combined and saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez('city_A_processed_data_optimized.npz', sequences=final_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for city b\n",
    "\n",
    "# load data\n",
    "mobility_data = pd.read_csv(\"hiroshima_challengedata.csv\")\n",
    "\n",
    "# Load POI distribution data\n",
    "poi_data = pd.read_csv(\"POIdata_cityB.csv\")\n",
    "\n",
    "# Load POI category mappings\n",
    "poi_categories = pd.read_csv(\"POI_datacategories.csv\", header=None, names=['category_name'])\n",
    "poi_categories['category_id'] = range(1, len(poi_categories) + 1)\n",
    "\n",
    "# Load Task 1 output (frequent itemsets)\n",
    "task1_output = pd.read_csv(\"frequent_itemsets_cityB.csv\")\n",
    "\n",
    "# Load Task 2 output (frequent patterns)\n",
    "task2_patterns = pd.read_csv('hiroshima_freq_subseq.csv')['Patterns'].apply(eval).tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge POI categories with distribution data\n",
    "poi_data = pd.merge(poi_data, poi_categories, left_on='category', right_on='category_id', how='left')\n",
    "\n",
    "# Filter mobility data to the first 30 days\n",
    "mobility_data['d'] = pd.to_numeric(mobility_data['d'], errors='coerce')\n",
    "mobility_data = mobility_data[mobility_data['d'] <= 30]\n",
    "\n",
    "# Remove rows with missing coordinates\n",
    "mobility_data = mobility_data[~((mobility_data['x'] == -999) & (mobility_data['y'] == -999))]\n",
    "\n",
    "# Calculate frequent support for each POI using Task 1\n",
    "def calculate_frequent_support(poi_category, task1_data):\n",
    "    return task1_data.loc[\n",
    "        task1_data['itemsets'].apply(lambda s: poi_category in s), 'support'\n",
    "    ].sum()\n",
    "\n",
    "poi_data['frequent_support'] = poi_data['category_name'].apply(\n",
    "    lambda x: calculate_frequent_support(x, task1_output)\n",
    ")\n",
    "\n",
    "# Aggregate POI data by grid\n",
    "poi_aggregated = poi_data.groupby(['x', 'y']).agg(\n",
    "    total_poi_count=('POI_count', 'sum'),\n",
    "    avg_frequent_support=('frequent_support', 'mean')\n",
    ").reset_index()\n",
    "\n",
    "# Merge POI features with mobility data\n",
    "mobility_data = pd.merge(\n",
    "    mobility_data, poi_aggregated,\n",
    "    on=['x', 'y'], how='left'\n",
    ").fillna(0)  # Fill missing POI features with 0\n",
    "\n",
    "# Precompute POI features as a NumPy array\n",
    "poi_features = ['total_poi_count', 'avg_frequent_support']\n",
    "poi_features_array = mobility_data[poi_features].to_numpy()\n",
    "\n",
    "# Sort the entire dataset once by user and time\n",
    "mobility_data = mobility_data.sort_values(['uid', 'd', 't']).reset_index(drop=True)\n",
    "\n",
    "# Convert task2_patterns to a set of tuples for fast lookup\n",
    "task2_patterns_set = set(map(tuple, task2_patterns))\n",
    "\n",
    "# Function to check if a sequence matches a pattern (optimized for set lookup)\n",
    "def match_pattern(sequence, patterns_set):\n",
    "    return 1 if tuple(map(tuple, sequence)) in patterns_set else 0\n",
    "\n",
    "# Function to process a single sequence for multiprocessing\n",
    "def process_sequence(data):\n",
    "    seq, match, poi_features_array = data\n",
    "    enriched_seq = [\n",
    "        np.hstack([step, poi_features_array[i], match]) for i, step in enumerate(seq)\n",
    "    ]\n",
    "    return enriched_seq\n",
    "\n",
    "# Group by user\n",
    "grouped = mobility_data.groupby('uid')\n",
    "\n",
    "# Initialize list for enhanced sequences\n",
    "enhanced_sequences = []\n",
    "# Sequence length for LSTM\n",
    "sequence_length = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing User Groups: 100%|██████████| 24906/24906 [00:42<00:00, 591.89it/s]\n"
     ]
    }
   ],
   "source": [
    "# Process each group with tqdm for progress tracking\n",
    "for _, group in tqdm(grouped, desc=\"Processing User Groups\"):\n",
    "    # Convert group to NumPy array for faster slicing\n",
    "    group_array = group[['x', 'y']].to_numpy()\n",
    "\n",
    "    # Generate sequences\n",
    "    num_sequences = len(group_array) - sequence_length\n",
    "    if num_sequences > 0:\n",
    "        sequences = np.lib.stride_tricks.sliding_window_view(group_array, (sequence_length, group_array.shape[1]))[:, 0]\n",
    "        matches = [match_pattern(seq, task2_patterns_set) for seq in sequences]\n",
    "        enhanced_sequences.extend(zip(sequences, matches))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize features\n",
    "max_x, max_y = mobility_data['x'].max(), mobility_data['y'].max()\n",
    "mobility_data['x'] = mobility_data['x'] / max_x\n",
    "mobility_data['y'] = mobility_data['y'] / max_y\n",
    "\n",
    "poi_features = ['total_poi_count', 'avg_frequent_support']\n",
    "mobility_data[poi_features] = mobility_data[poi_features] / mobility_data[poi_features].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating LSTM Sequences: 100%|██████████| 9859291/9859291 [04:12<00:00, 39047.83it/s]\n"
     ]
    }
   ],
   "source": [
    "final_sequences = []\n",
    "for seq, match in tqdm(enhanced_sequences, desc=\"Creating LSTM Sequences\"):\n",
    "    # Combine sequence with precomputed POI features and pattern match\n",
    "    enriched_seq = [\n",
    "        np.hstack([step, poi_features_array[i], match])  # Use preloaded features\n",
    "        for i, step in enumerate(seq)\n",
    "    ]\n",
    "    final_sequences.append(enriched_seq)\n",
    "\n",
    "final_sequences = np.array(final_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez('city_B_processed_data_optimized.npz', sequences=final_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# for city c\n",
    "\n",
    "# load data\n",
    "mobility_data = pd.read_csv(\"sapporo_challengedata.csv\")\n",
    "\n",
    "# Load POI distribution data\n",
    "poi_data = pd.read_csv(\"POIdata_cityC.csv\")\n",
    "\n",
    "# Load POI category mappings\n",
    "poi_categories = pd.read_csv(\"POI_datacategories.csv\", header=None, names=['category_name'])\n",
    "poi_categories['category_id'] = range(1, len(poi_categories) + 1)\n",
    "\n",
    "# Load Task 1 output (frequent itemsets)\n",
    "task1_output = pd.read_csv(\"frequent_itemsets_cityC.csv\")\n",
    "\n",
    "# Load Task 2 output (frequent patterns)\n",
    "task2_patterns = pd.read_csv('sapporo_freq_subseq.csv')['Patterns'].apply(eval).tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge POI categories with distribution data\n",
    "poi_data = pd.merge(poi_data, poi_categories, left_on='category', right_on='category_id', how='left')\n",
    "\n",
    "# Filter mobility data to the first 30 days\n",
    "mobility_data['d'] = pd.to_numeric(mobility_data['d'], errors='coerce')\n",
    "mobility_data = mobility_data[mobility_data['d'] <= 30]\n",
    "\n",
    "# Remove rows with missing coordinates\n",
    "mobility_data = mobility_data[~((mobility_data['x'] == -999) & (mobility_data['y'] == -999))]\n",
    "\n",
    "# Calculate frequent support for each POI using Task 1\n",
    "def calculate_frequent_support(poi_category, task1_data):\n",
    "    return task1_data.loc[\n",
    "        task1_data['itemsets'].apply(lambda s: poi_category in s), 'support'\n",
    "    ].sum()\n",
    "\n",
    "poi_data['frequent_support'] = poi_data['category_name'].apply(\n",
    "    lambda x: calculate_frequent_support(x, task1_output)\n",
    ")\n",
    "\n",
    "# Aggregate POI data by grid\n",
    "poi_aggregated = poi_data.groupby(['x', 'y']).agg(\n",
    "    total_poi_count=('POI_count', 'sum'),\n",
    "    avg_frequent_support=('frequent_support', 'mean')\n",
    ").reset_index()\n",
    "\n",
    "# Merge POI features with mobility data\n",
    "mobility_data = pd.merge(\n",
    "    mobility_data, poi_aggregated,\n",
    "    on=['x', 'y'], how='left'\n",
    ").fillna(0)  # Fill missing POI features with 0\n",
    "\n",
    "# Precompute POI features as a NumPy array\n",
    "poi_features = ['total_poi_count', 'avg_frequent_support']\n",
    "poi_features_array = mobility_data[poi_features].to_numpy()\n",
    "\n",
    "# Sort the entire dataset once by user and time\n",
    "mobility_data = mobility_data.sort_values(['uid', 'd', 't']).reset_index(drop=True)\n",
    "\n",
    "# Convert task2_patterns to a set of tuples for fast lookup\n",
    "task2_patterns_set = set(map(tuple, task2_patterns))\n",
    "\n",
    "# Function to check if a sequence matches a pattern (optimized for set lookup)\n",
    "def match_pattern(sequence, patterns_set):\n",
    "    return 1 if tuple(map(tuple, sequence)) in patterns_set else 0\n",
    "\n",
    "# Function to process a single sequence for multiprocessing\n",
    "def process_sequence(data):\n",
    "    seq, match, poi_features_array = data\n",
    "    enriched_seq = [\n",
    "        np.hstack([step, poi_features_array[i], match]) for i, step in enumerate(seq)\n",
    "    ]\n",
    "    return enriched_seq\n",
    "\n",
    "# Group by user\n",
    "grouped = mobility_data.groupby('uid')\n",
    "\n",
    "# Initialize list for enhanced sequences\n",
    "enhanced_sequences = []\n",
    "# Sequence length for LSTM\n",
    "sequence_length = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing User Groups: 100%|██████████| 19950/19950 [00:35<00:00, 560.96it/s]\n"
     ]
    }
   ],
   "source": [
    "# Process each group with tqdm for progress tracking\n",
    "for _, group in tqdm(grouped, desc=\"Processing User Groups\"):\n",
    "    # Convert group to NumPy array for faster slicing\n",
    "    group_array = group[['x', 'y']].to_numpy()\n",
    "\n",
    "    # Generate sequences\n",
    "    num_sequences = len(group_array) - sequence_length\n",
    "    if num_sequences > 0:\n",
    "        sequences = np.lib.stride_tricks.sliding_window_view(group_array, (sequence_length, group_array.shape[1]))[:, 0]\n",
    "        matches = [match_pattern(seq, task2_patterns_set) for seq in sequences]\n",
    "        enhanced_sequences.extend(zip(sequences, matches))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize features\n",
    "max_x, max_y = mobility_data['x'].max(), mobility_data['y'].max()\n",
    "mobility_data['x'] = mobility_data['x'] / max_x\n",
    "mobility_data['y'] = mobility_data['y'] / max_y\n",
    "\n",
    "poi_features = ['total_poi_count', 'avg_frequent_support']\n",
    "mobility_data[poi_features] = mobility_data[poi_features] / mobility_data[poi_features].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating LSTM Sequences: 100%|██████████| 7517188/7517188 [03:17<00:00, 38102.04it/s]\n"
     ]
    }
   ],
   "source": [
    "final_sequences = []\n",
    "for seq, match in tqdm(enhanced_sequences, desc=\"Creating LSTM Sequences\"):\n",
    "    # Combine sequence with precomputed POI features and pattern match\n",
    "    enriched_seq = [\n",
    "        np.hstack([step, poi_features_array[i], match])  # Use preloaded features\n",
    "        for i, step in enumerate(seq)\n",
    "    ]\n",
    "    final_sequences.append(enriched_seq)\n",
    "\n",
    "final_sequences = np.array(final_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez('city_C_processed_data_optimized.npz', sequences=final_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
